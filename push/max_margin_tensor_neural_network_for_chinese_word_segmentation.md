---
title: post test
date: 2017-11-27 21:40:30
tags:
mathjax: true
---
### 摘要
最近，神经网络因其能够减轻自然语言处理任务中的特征选择而备受关注。在这篇文章中，我提出了一个用于中文分词的模型叫做 Max-Margin Tensor Nerual Network(MMTNN)。该模型通过利用标签嵌入(tag embedding)和基于张量的转换，能过对标签和上下文间的复杂关系进行建模。本文你还提出了一个新的张量分解方法来防止过拟合。本模型在基准数据集上取得了比以前的神经网络更好的结果，而且在有少量人工特征的情况下效果更好。尽管中文分词是一个特殊的例子，本模型可以很容易的应用到其他序列标注任务中。
### 1.前言
中文不像英文或其他西方语言能够用空格来确定词的边界，所以分词是中文处理中基础且十分重要的预处理方法。大多数将这个问题看作一个序列标注问题，为每个字符分配一个标签来指示它在词语中的位置。这些系统因其能够利用大量的人工特征而变得十分有效。但这些模型需要设计特征，并且当特征过多时导致模型过大而不容易实现还倾向于过拟合训练语料。

最近，神经网络因其能够减小特征工程的工作量而收到重视。Collobert etal.开发的SENNA系统在各种英语的序列标注任务中接近或超过了已有的其他系统。Zheng et al.将SENNA系统用于中文分词和词性标注任务，提出了一个类似感知机的算法，在稍微降低性能的情况下提高了训练的速度。

在这些神经网络模型中，存在不能很好的对标签之间关系、标签和字符之间关系、字符之间关系建模的问题。在传统基于特征的线性模型中，这些关系都被当作特征建模。以“打篮球”为例，假如我们为字符 $C_0$ = “蓝”，可能的特征有：
$$ f_1=\left\{
\begin{aligned}
1  && C_-1=“打”\ and\ C_1=“球”\ and\ y_0="B"\\
0  &&else \\
\end{aligned}
\right.
$$
$$ f_2=\left\{
\begin{aligned}
1  && C_0=“蓝”\ and\ y_0=“B”\ and\ y_-1="S"\\
0  &&else \\
\end{aligned}
\right.
$$
为了获取到更多的关系，研究人员根据语言直觉和统计信息设计大量的特征。然而在以前的神经网络模型仅仅通过简单转换和单一非线性转换很难捕获这些特征。为了解决这个问题，我们提出了MMTNN模型，该模型能够通过标签嵌入和张量转换很好的对标签和字符之间的关系进行建模。又给出了一种利用张量分解防止过拟合的方法。我们将模型应用到PKU和MSRA数据集的分词任务中，取得了比其他神经网络模型更好好的效果。

尽管本文主要关注在不使用特征工程的情况下能得到的最好效果，但是将深度学习应用到自然语言处理依然是一个新的尝试，在不使用额外特征的情况下很难超越传统方法。与Mansur et al.一样我们尝试在使用少量特征时模型能取得多大的效果。我们在模型中加入了Bigram（二元语法模型）特征，取得了比其他模型更好的性能。

本文的主要贡献如下：
1. 我们提出了一个Max-Margin Tensor Neural Network 模型，并将该模型应用到中文分词任务中，取得比其他模型更好的效果。
2. 我们提出了一个张量分解方法，能够将其分解为两个低秩的矩阵，它不仅提高了模型的速度还能够防止过拟合。
3. 与使用大量手工特征模型相比，本模型使用极少的特征就能得到很好的性能。
4. 该模型可以很容易的推广到其他序列标注任务。
接下来文章分为以下几部分，第2节详细介绍神经网络模型，第3节介绍本文提出的模型，第4节实验，第5节介绍相关的工作，第6节总结。

### 2.传统神经网络
##### 2.1 Lookup Table 查找表
对符号数据进行分布式表示（distributed representation）是神经网络的一个重要功能。它是Hinton在1986年提出的，一直以来都是研究的热点问题。形式上，在中文分词任务中都有一个大小为|$D$|的字符字典$D$。除非其它特殊情况，一般字符字典都是从训练数据集中提取出来的对不知道的字符用一个在字典中没有出现过的字符表述。每一个字符 $c \in D$ 都用一个维度为 $d$ 的实数向量$Embed(c ) \in R^d$表示。字符被嵌入到一个举证中 $M \in R^{d\times|D|}$ ，对每个属于 $D$ 的字符 $c$ 都有一个索引值 $k$ ，每个字符对应的嵌入向量可以通过查找表找到如下图所示：

$$
Embed(c) = Me_k
$$
![](https://thumbnail10.baidupcs.com/thumbnail/80458f9bb877bfb89282a9b59efd5d55?fid=3459204613-250528-705193623887497&time=1511697600&rt=sh&sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-MuY0VKRzNjOv0CF76hn4Y%2B0Y6%2BE%3D&expires=8h&chkv=0&chkbd=0&chkpc=&dp-logid=7650148787353827262&dp-callid=0&size=c10000_u10000&quality=90&vuk=-&ft=video)

这里 $e_k \in R ^{|D|}$ 是一个二值向量第 $k$ 个位置为1其它位置都是0,这个查找表层可以看作是一个投影层，根据字符的索引值将字符转化为向量。这个嵌入矩阵(embedding matrix) 用小的随机数初始化，并且用反向传播训练。
##### 2.2 标签得分
通常的标注是基于窗口的方法，该方法认为字符的标签是基于它的邻近字符的。给定一个句子 $c_{[1:n]}$ 窗口大小为 $w$ 从 $c_1$ 到 $c_n$ 依次划过。在实验中我们设置的大小为5，正如上图所示，在位置 $c_i$ , $1\le i \le n$，  上下文字符都被输入到 lookup table 层中，超过句子边界的用两个特殊字符表示 “start” 和 “end”，把从 lookup table 中找到的向量连接成一个向量， $H_1 = w\cdot d$ 是 layer1 的大小。接下来将 $a$ 做线性变换，再用激活函数比如 $tanh$ 得到 Layer1 的输出：
$$
h=g(W_1a + b)
$$
$W_1 \in R^{H_1 \times H_2}$ , $b_2 \in R^{|T| \times1}$。$H_2$ 是 Layer2 的参数，给定几个大小为 |$T$| 的标签，对应的预测输出如下式：
$$
f(t|c_{[i-2:i+2]}) = W_2h + b_2
$$
函数 $f$ 为标签的得分，在中文分词中常用的标签为: BEMS，B 是词的开始，M是词的内部， E 是词的结尾， S 是单个的词。

##### 2.3建模和推理
尽管和以前的模型有相似之处，但是它们用了不同的建模和推理方法。Mansur et al. 将中文分词看做一系列分类任务在每个位置把标签的得分用 softmax 转化为概率：
$$
p(t_i|c_{i-2:i+2}) = \frac{exp(f(t_i|c_{[i-2:i+2]}))}{\sum_{t'}{exp(f(t'|c_{i-2:i+2}))}}
$$
该模型用极大化标签数据对数似然，显然该模型不能获得标签间的相互依赖关系，也不能在全局上推理标签序列。

为了建模标签间的依赖关系，以前的模型提出了一个用 $A_{ij}$ 描述从标签 $i \in T$ 到标签 $j \in T$ 的转移得分。对一个输入的句子 $c_{[1:n]}$ 序列和对应的标签序列 $t_{1:n}$ 句子的对应的得分是转移得分和网络得分的总和：
$$
s(c_{[1:n]} , t_{[1:n]} , \theta) = \sum_{i=1}^n(A_{t_{i-1}} + f_{\theta}(t_i|c_{[i-2:i+2]}))
$$
其中 $f_{\theta}(t_i|c_{[i-2:i+2]})$ 是在参数 $\theta = (M,A,W_1,b_1,W_2,b_2)$ 下第 $i$ 个字符对应的标签 $t_i$ 的得分。Zheng et al.受到 Mansur et al.启发提出一个类似感知机的训练算法。和 Mansur et al.的相比，他们的模型是一个在句子层面的训练和推理。
这些模型的一个缺点是标签间的关系和神经网络是分开建模的。简单的标签转移忽略了上下文字符的影响，无法捕获标签和字符上下文间的依赖关系，并且简单的非线性变换也无法对分词任务中复杂的依赖关系建模。

### 3.Max-Margin Tensor Neural Network
##### 3.1 标签嵌入(Tag Embedding)
为了更好的对标签间的关系进行建模，本模型使用分布式表示而没有使用传统的符号表示。和字符的嵌入类似，给定一个大小固定的标签集合 $T$ ，表签的嵌入被保存在一个嵌入矩阵 $L \in R^{d\times|T|}$ 中,d是向量的维度，索引为 $k$ 的标签 $t \in T$ 对应的嵌入 $Embed(t)$ 可以通过查找表找到：
$$
Embed(t) = Le_k
$$
$e_k \in R^{|T|\times1}$ 是一个二值的向量除了 $k$ 对应的位置为 1 以外，其他都为 0 ,标签对应嵌入向量被随机的初始化，然后用反向传播来优化。如下图所示：
![](https://thumbnail10.baidupcs.com/thumbnail/f84f9b928544c7fd23c792dd2ee619cf?fid=3459204613-250528-116157092144579&time=1511744400&rt=pr&sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-v9mUqtQ55fFQeQaa7b526GdIuoo%3d&expires=8h&chkbd=0&chkv=0&dp-logid=7663207781982535450&dp-callid=0&size=c10000_u10000&quality=90&vuk=3459204613&ft=image)
假设我们现在在句子中的第 $i$ 个字符，除了字符的嵌入外还考虑了前几个标签的嵌入。考虑到速度，本文只考虑了前一个标签，但是长距离的也是可以使用的。Layer1 层的向量变成了由字符嵌入和标签嵌入拼接成，用这种方法将标签的表示直接拼接到神经网络中，就可以很方便的在接下来的网络层中对标签与字符和标签间的依赖关系进行建模。转移矩阵不是本模型所必须的，因为通过把标签的嵌入假如神经网络，使模型有了对字符间与标签间的依赖关系表示的能力。所以得分公式变成了下式：
$$
s(c_{[i:n]}, t_{[i:n]},theta) = \sum_{i=1}^{n}f_{\theta}(t_i|c_{[i-2:i+2]},t_{i-1})
$$
其中 $f_{\theta}(t_i|c_{[i-2:i+2]},t_{i-1})$ 神经网络是在参数 $\theta$ 下第 $i$ 个字符对应的标签 $t_i$ 的得分。模型在句子层面上训练，得到全局的参数。

##### 3.2 Tensor Neural Network
一个张量可以用来描述向量、标量和其他张量间的几何关系，它可以看作是由实数组成的多维数组。可以对数据间的多种关系进行建模，基于张量的模型已经在很多任务中得到了应用。

在中文分词任务中对标签和标签间关系、标签和字符间关系、字符和字符间关系进行建模是十分重要的。在线性模型中这些关系被当作特征处理。在传统神经网络中，输入的嵌入仅使用非线性函数很难表示出它们之间的复杂相互作用关系。结合张量的特点，我们给出了一个对输入向量基于张量的转换。我们使用一个3阶的张量 $V^{[1:h_2]} \in R^{H_2 \times H_1 \times H_1}$ 来对它们之间的相互作用建模， $H_2$ 是 Layer2 层的大小，$H_1 = (w + 1)\cdot d$ 是 Layer1 的大小，该转换如下如所示：
![](https://thumbnail10.baidupcs.com/thumbnail/bcedaff0f685cc9ef58b3f75d37f9a8d?fid=3459204613-250528-204074599931464&time=1511748000&rt=pr&sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-lAWn8GvITn3uON52W487MtTObM0%3d&expires=8h&chkbd=0&chkv=0&dp-logid=7663918442869867375&dp-callid=0&size=c10000_u10000&quality=90&vuk=3459204613&ft=image)
张量计算的输出是一个向量 $z \in R^{H_2}$ 每个 $z_i$ 都是根据张量 $V^{[i]} \in R^{H_1 \times H_2}$ 的双线性变换结果:
$$
z = a^TV^{[1:H_2]};z_i = a^TV^{[i]}a=\sum_{j,k}V_{jk}^{[i]}a_ja_k
$$
由于 $a$ 是由字符嵌入和标签嵌入拼接来的，可以写作：
$$
z_i = \sum_{p,q}\sum_{j,k}V_{p,q,j,k}^{[i]}E_j^{[p]}E_k^{[q]}
$$
$E_j^{[p]}$ 是第 $j$ 个元素在查找表中的第 $p$ 个嵌入， $V_{(p,q,j,k)}^{[i]}$ 是 $E_j^{[p]}$ 和 $E_k^{[q]}$ 在 $V^{[i]}$ 中的系数。我们可以看到对张量的每个切片 $i$ 嵌入通过双线性的变换捕获字符和标签间的相互关系。字符嵌入和标签嵌入的乘法可以看作是基于特征模型的手工特征的组合，在该模型中能够通过张量和嵌入自动学习得到，直观的，我们可以认为张量的每个切片都捕获了特定的字符标签间作用和字符字符间作用。组合张量的结果和线性变换，Layer2可以定义为：
$$
h = g(a^TV^{[1:H_2]}a + W_1a + b_1)
$$
##### 3.3 Tensor Factorization 张量分解
尽管基于张量的转换可以有效的捕获这些关系，但是将张量引入神经网络来解决序列标注问题会使模型变得十分缓慢，在没有矩阵优化的算法中非线性转换的复杂度为  $O(H_1H_2)$ ,然而有了张量后的复杂度变成了 $O(H_1^2H_2)$ ，变慢了 $H_1$ ，此外张量的假如是模型的参数增加，增大了模型产生过拟合的几率。为了解决这个问题，我们给出一个分解方法，将张量分解为两个低秩矩阵，既一个张量的切片 $V^{[i]} \in R^{H_1 \times H_2}$ 分解为两个低秩的矩阵 $P^{[i]} \in R^{H_1 \times r}$ 和 $Q^{[i]} \in R^{r \times H_1}$ ：
$$
V^{[i]} = P^{[i]}Q^{[i]},1 \le i \le H_2
$$
其中 $r \ll H_1$ 是因子的个数，将该式与上面的式子结合得到：
$$
h = g(a^T P^{[1:H_2]}Q^{[1:H_2]}a + W_1a + b_1)
$$
下图展示了对分解的张量的每个切片的操作：
![](https://thumbnail10.baidupcs.com/thumbnail/c5f61d6d304fa665b681f7d438ea672f?fid=3459204613-250528-633336671023504&time=1511758800&rt=pr&sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-ojK0v8EWqSwT1mh78GR6kgEOL54%3d&expires=8h&chkbd=0&chkv=0&dp-logid=7666828337097895364&dp-callid=0&size=c10000_u10000&quality=90&vuk=3459204613&ft=image)

首先将向量 $a$ 投影到两个 $r$ 维的向量 $f_1$ 和 $f_2$ ，然后每个输出 $z_i$ 都是向量 $f_1$ 与 向量 $f_2$ 的点积。复杂度变为了 $O(rH_1H_2)$ 。只要 $r$ 足够小，分解张量的运算会比没有分解的快，并且参数也会变少，可以避免模型的过拟合。
##### 3.4 Max-Margin Training
我们使用最大边缘准则来训练模型，该准则关注模型决策边界的鲁棒性提供了可选的概率的和基于似然估计的方法。我们使用 $Y(x_i)$ 表示在给定句子 $x_i$ 的情况下可能的标签序列，对 $x_i$ 正确的标注是 $y_i$ 。模型的参数为 $\theta = \{W_1,b_1,W_2,b_2,M,L,P^{[1:H_2]},Q^{[1:H_2]}\}$ ，定义在给定正确标签序列 $y_i$ 的预测标签序列 $\widehat{y}$ 的边缘损失为：
$$
\Delta(y_i,\widehat{y}) = \sum_j^nk1\{y_i,j\ne \widehat{y}_j\}
$$
其中 $n$ 是句子 $x_i$ 的长度， $k$ 是打折参数。损失值和序列中标注错误的标签数成正比。对一个给定的训练实例 $(x_i,y_i)$ 我们取寻找标签序列得分最高的：
$$
y^* = argmaxs(x_i,\widehat{y},\theta)
$$
最大边缘训练的目的是得分最高的标注序列是正确的： $y^* = y_i$的得分将会比其他的可能的序列高：
$$
s(x,y_i,\theta) \ge s(x,\widehat{y},\theta) + \Delta(y_i,\widehat{y})
$$
这个式子可以改为 $m$ 个训练数据加归一化后的函数：
$$
J(\theta) = \frac{1}{m}\sum_{i=1}^{m}l_i(\theta) + \frac{\lambda}{2}\parallel \theta \parallel^2
$$
$$
l_i(\theta) = \max \limits_{\widehat{y} \in Y(x_i)}{(s(x,\widehat{y},\theta) +  \Delta(y_i,\widehat{y}) - s(x_i,y_i,\theta))}
$$
通过训练，正确的标注序列得分增加，不正确的高分序列标注得分降低。

这个目标函数不是十分复杂，我们使用一个一般化的梯度下降方法：次梯度方法（subgradient method）计算类似梯度。如下:
$$
\frac{\varphi J}{\varphi \theta}  = \frac{1}{m}\sum_i(\frac{\varphi(x_i,\widehat{y}_{max},\theta)}{\varphi\theta} - \frac{\varphi s(x_i,y_i,\theta)}{\varphi \theta}) + \lambda \theta
$$
其中 $\widehat{y}_{max}$ 是得分最高的标签序列。使用 AdaGrad 和 minibatchs来最小化目标。参数更新如下：
$$
\theta_{t,i} = \theta_{t-1,i} - \frac{\alpha}{\sqrt{\sum_{r=1}^{t}g^2_{r,i}}}g_{t,i}
$$
$\alpha$ 是学习率， $g_r \in R^{|\theta_i|}$ 是在时间 $t$ 参数 $\theta_i$ 的次梯度。
### 4 实验
##### 4.1 数据和模型选择
我们使用 PKU 和 MSRA 数据来测试模型，这些数据也是其他神经网络模型使用的。数据的详细介绍如下：
![](https://thumbnail10.baidupcs.com/thumbnail/146366e75df8366ef8fca3ca4ca565f0?fid=3459204613-250528-162212207456600&time=1511769600&rt=pr&sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-le%2b2sIUkqnGiPWgfm9MVIdegMic%3d&expires=8h&chkbd=0&chkv=0&dp-logid=7669095180852727598&dp-callid=0&size=c10000_u10000&quality=90&vuk=3459204613&ft=image)

我们用准确率，召回率，F1值和未登录词召回率来评估模型。

我们用前 90% 的句子作为训练数据，剩下的 10% 句子作为 dev数据， minibatch 大小为 20 ，当隐藏层的数量过大时会限制模型的性能，当为 50 时模型的速度和性能最好。字符嵌入维度大小为25时取得了最好的性能，且速度比 50 和 100 维的快。我们也验证了张量分解因子的个数，性能并不都是好的，当因子个数大于 10 个时训练时间增加很长，我们推测是因子多是参数变多导致性能下降。最后的参数设置如下：

![](https://thumbnail10.baidupcs.com/thumbnail/a54f0bd250a77aeac719c2dcc548f079?fid=3459204613-250528-174562657789209&time=1511769600&rt=pr&sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-AWIXIAfvwsoF5SqbS%2f4xMs09dNs%3d&expires=8h&chkbd=0&chkv=0&dp-logid=7669384593686294992&dp-callid=0&size=c10000_u10000&quality=90&vuk=3459204613&ft=image)

##### 4.2 实验结果
我们将模型同基于CRF的中文分词方法进行比较，CRF模型的输入特征是上下文字符，我们使用开源工具 [CRF++](http://taku910.github.io/crfpp/) 来训练该模型。所有的神经网络模型都使用Max-Margin 方法，下表给出了实验结果。
|          | P     | R | F | OOV|
| :------ | :----- |:----- |:----- |:----- |
| CRF | 87.8 | 85.7 | 86.7 | 57.1 |
| NN | 92.4 | 92.2 | 92.3 | 60.0 |
| NN+Tag Embed | 93.0 | 92.7 | 92.9 | 61.0 |
| MMTNN | 93.7 | 93.4 | 93.5 | 64.2 |
从表中可以看出，使用 tag enbedding 可以是 F 值增加 0.6% ，未登录词 recall 值增加1.0% 说明可以很好的对 标签之间关系和字符标签间关系进行建模。通过使用张量模型的性能进一步得到提升，说明基于张量的转换能够比简单的非线性转换学习到更多的关系信息。从表中还可以看出，在只使用一元特征的情况下，基于神经网络的模型比基于 CRF 的模型好。神经网络模型和 CRF 相比有两个不同：第一，离散的特征被连续的字符嵌入替代;第二，非线性转换用来发现更高层面上的表示。事实上 CRF 模型可以看作是一个没有非线性函数的神经网络。Wand 和 Manning 的实验表明只有在使用分布式表示的情况个下非线性函数能够使模型性能更高。为了解释分布式表示能够比离散特征得到更多信息，我们选择几个词，利用欧式距离公式算出距离其最近的五个词，得到下表：
| 一    | 李     | 。     |
| :------------- | :------------- |:------------- |
| 二       | 赵       | ，      |
| 三       | 蒋       | ：      |
| 四       | 孔       | ？      |
| 五       | 冯       | “       |
| 六       | 吴       | 、      |
第一列都是中文的数字，第二列都是中文的姓，第三列都是中文的标点符号。因此，和离散特征相比，分布式表示可以捕获到字符间的语法的和语义的相似性。在一些词没有出现的情况下模型仍然表现很好。

我们还将模型同以前的神经网络模型进行对比，如下所示我们的模型取得了比其他模型更高的 F 值：
![](https://thumbnail10.baidupcs.com/thumbnail/480ac015c4e553e387aa047518da76ec?fid=3459204613-250528-569394838344233&time=1511773200&rt=pr&sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-0Ng%2f1%2bqP1grEQqtRd1ZA18N7RUc%3d&expires=8h&chkbd=0&chkv=0&dp-logid=7670275316033317822&dp-callid=0&size=c10000_u10000&quality=90&vuk=3459204613&ft=image)
##### 4.3 无监督的预训练
以前的工作显示模型使用通过在大量的没有标注的数据上预训练得到的查找表来初始化查找表能提高模型的性能，而不是随机初始化的查找表。 Mikolov et al.指出预训练的词嵌入可以捕获语义和语法信息，比如英语中的 $king - man + woman \thickapprox queen$ 。有很多中从无标签数据中得到词嵌入的方法：Mansur et al. 使用 Bengip et al.的模型从基于神经网络的模型嗯中得到词嵌入。Zheng et al.采用Collobert et al.模型，这个模型是一个神经网络，给语料库中的一定窗口大小的词大的得分，给随机替换其中值的小的得分。 Mikolov et al.提出了一个快速训练的模型 skip-gram word2vec ，该模型努力最大化一个词基于句子中其他词的分类。在本文中我们使用 word2vec 因为它不仅和其他模型相比得到的此表示没有多到差别，还训练速度十分快。我们使用 Chinese Giga-word corpus 预训练。如上图所示，使用预训练的词向量得到了很高的值。
##### 4.4 Minimal Feature Engineering
尽管我们在研究没有使用特征工程的情况下能做到多好，但是深度学习在自然语言处理中的应用还是一个新的尝试，很难超越最好的在不使用特征工程的情况下。为了将特征引入神经网络， Mansur et al. 提出了基于特征的神经网络将上下文特征表示为特征嵌入（feature embedding），其方法和字符嵌入是一样的。一般地，我们把提取的特征组合成一个字典 $D_f$ ，对每个特征 $ f \in D_f $
 用一个 $d$ 维的向量表示就是特征嵌入。沿着这个思路，我们想试试在使用少量特征的情况下，模型能取得多好的性能。

 中文分词的典型特征是二元文法（bigram）特征。句子中第 $i$ 个字符的二元文法特征为 $c_kc_{k+1}(i-3 <k<i+2)$ 。在我们的模型中二元文法特征在窗口的上下文中取得，然后转化为对应的特征嵌入（向量），将其与字符和标签的嵌入拼接作为 Layer1 输入到 Layer2 中。在 Mansur et al.的论文中使用预训练的特征嵌入向量，取得了很好的效果。我们仅仅预训练字符嵌入向量，二元文法的预训练时间很长，于是我们使用字符嵌入的平均值得到二元文法嵌入向量，如果也训练二元文法嵌入应该能取得更好的效果。下图展示了我们的分词效果和其他很好的模型。

 ![](https://thumbnail10.baidupcs.com/thumbnail/98e6e16d986eef3005d62cc1254a14f8?fid=3459204613-250528-39061525855345&time=1511784000&rt=pr&sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-J%2fguZcSDe5dvS1fnnWNrlUIi5vM%3d&expires=8h&chkbd=0&chkv=0&dp-logid=7673670920064020565&dp-callid=0&size=c10000_u10000&quality=90&vuk=3459204613&ft=image)


从图中可以看出加入 bigram 特征后取得了比其他模型更高的 F 值。在只使用 bigram 特征的情况下，我们的模型取得了跟其他使用复杂特征模型有竞争力的效果。比如,Sun et al.在模型中加入了基于词的特征；Zhang et al.在模型使用了8中特征如互信息、类别、从领域语料库和领域外语料库提取动态统计信息。本文不过多研究特征工程，所以没有使用更多的特征。
### 5 相关工作
自然语言处理社区已经中文分词的研究付出了相当大的努力。Xue 提出了最受欢迎的方法，将中文分词看作序列标注问题。大多数以前的系统用线性统计模型解决这个问题，它们使用精心设计的特征比如bigram 特征、标点信息、统计信息等。最近，研究者们使用自动学习特征的神经网络模型来解决分词任务。本论文就是属于这一类的，我们的模型可以很好的对标签和上下文字符间的关系进行建模，捕获跟多的语义信息。

基于张量转换的方法通用被应用到其他神经网络模型中来获取数据间的复杂关系。比如Socher et al.在情感分析中使用基于张量的函数来获取成分之间的语义信息，但是，由于它们使用小的张量，他们没有遇到我们在分词任务中时间花费大和过拟合的问题，这是我们为什么使用张量分解来减少计算花费和避免过拟合的原因。

最近提出了很多张量分解的方法，来降维。比如Milti-Relational LSA 。和 LSA 相似使用张量分解的方法得到近似的低秩张量。该方法还被用到协同过滤和目标识别。我们的方法和它们类似，但使用了不同的分解方法。通过把张量分解加入到序列标注神经网络模型中，提高了模型的训练速度，降低了过拟合风险
### 6 结论
本文提出了一个 Max-Margin Tensor Neural Network 能够对标签和字符上下文间的相互关系进行建模。还给出了一个张量分解方法，能够提高模型的效率防止过拟合。在数据集上的实验显示我们的模型比以前的模型好。在未来打算扩展模型使其应用到其他预测问题上。
